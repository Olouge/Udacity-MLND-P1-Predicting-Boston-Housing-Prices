{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "3) Analyzing Model Performance\n",
    "\n",
    "4) Model Prediction\n",
    "\n",
    "Model makes predicted housing price with detailed model parameters (max depth) reported using grid search. Note due to the small randomization of the code it is recommended to run the program several times to identify the most common/reasonable price/model complexity.\n",
    "Compare prediction to earlier statistics and make a case if you think it is a valid model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Statistical Analysis and Data Exploration"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Number of data points (houses) : 506\n",
    "Number of data points (houses)? : 506\n",
    "Number of features? : 13\n",
    "Minimum and maximum housing prices? : min. 5, max. 50\n",
    "Mean and median Boston housing prices? : 22.53\n",
    "Standard deviation? : 21.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2-1. Which measure of model performance is best to use for predicting Boston housing data and analyzing the errors? Why do you think this measurement most appropriate? Why might the other measurements not be appropriate here?\n",
    "\n",
    "\"MSE\" is best to use for predicting Boston housing data and analysing the data. This is because 'DecisionTreeRegressor', used in boston_housing.py, can only use MSE to measure the quality of a split, so it makes sense to use a same criterion/metric, \"MSE\", in model selection steps.\n",
    "\n",
    "# As in DecisionTreeRegressor's help, MSE is the only supported criterion.\n",
    " |  criterion : string, optional (default=\"mse\")\n",
    " |      The function to measure the quality of a split. The only supported\n",
    " |      criterion is \"mse\" for the mean squared error, which is equal to\n",
    " |      variance reduction as feature selection criterion."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2-2. Why is it important to split the Boston housing data into training and testing data? What happens if you do not do this?\n",
    "\n",
    "In general, models must be trained by training data to improve their fitness to training data, and must be validated by testing data to check whether they have good estimates for unseen data or not. If we do not have  any testing data, it is impossible to validate performance of models for unseen data and to evaluate how models overfit to training data unless we get new data. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2-3. What does grid search do and why might you want to use it?\n",
    "\n",
    "In 'boston_housing.py', 'DecisionTreeRegressor' is used as a learning method and it takes 'max_depth' as a hyper parameter.\n",
    "\n",
    "Grid search estimates appropriate hyper parameters within given range of parameters based on performance metrics. When we have candidate parameters but don't know which ones should be used, it makes sense to use grid search to estimate best performed hyper parameters."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2-4. Why is cross validation useful and why might we use it with grid search?\n",
    "\n",
    "Cross validation helps us to measure average performance to unseen data (testing data), and this measure is almost essential for models to avoid overfitting. If we use it with grid search, we can select best performed hyper parameters based on average performance to unseen data, which means models with selected hyper parmaeters will well-perform to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Analyzing Model Performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3-1. Look at all learning curve graphs provided. What is the general trend of training and testing error as training size increases?\n",
    "\n",
    "The general trend of training and test error:\n",
    "  [a] 0 <= training_error < test_error\n",
    "  [b] training_error[i] < training_error[j] (0 <= i < j <=n)\n",
    "  [c] test_error[i] < test_error[j] (0 <= i < j <= n)\n",
    "\n",
    "The general trend of the error as training size increases:\n",
    "  [x] Difference between training_error and test_error is decreased.\n",
    "  [y] Trends of [a] and [b] shown in above become small."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3-2. Look at the learning curves for the decision tree regressor with max depth 1 and 10 (first and last learning curve graphs). When the model is fully trained does it suffer from either high bias/underfitting or high variance/overfitting?\n",
    "\n",
    "The regressor with max depth 1 suffers from high bias/underfitting because both test and training error are much higher than ones with max depth 10.\n",
    "On the other hand, the regressor with max depth 10 suffers from high variance/overfitting because test error fluctuates and test error is much higher than training error even if training data size is bigger ."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3-3. Look at the model complexity graph. How do the training and test error relate to increasing model complexity? Based on this relationship, which model (max depth) best generalizes the dataset and why?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. training error: increase, test error: decrease\n",
    "2. 1: high bias/underfitting 10: high variance/overfitting\n",
    "3. training error can be decreased by bottom, but test error is not decreased after 5 depth. Since increasing model complexity means overfitting, the complexity should be decreased where test error is stabilized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Model Prediction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Scoring function for regression defined in a sklearn library.\n",
    "- mean_squared_error\n",
    "- mean_absolute_error\n",
    "- median_absolute_error\n",
    "- explained_variance_score: 'variance_weighted' / 'uniform_average'\n",
    "\n",
    "In statistics, explained variation measures the proportion to which a mathematical model accounts for the variation (dispersion) of a given data set. < We want to predict house price\n",
    "\n",
    "- r2_score: 'variance_weighted' / 'uniform_average'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.957173447537\n",
      "[ 0.96774194  1.        ]\n",
      "0.990322580645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "print explained_variance_score(y_true, y_pred)  \n",
    "\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "print explained_variance_score(y_true, y_pred, multioutput='raw_values')\n",
    "\n",
    "print explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3125\n",
      "7.296875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95717344753747324"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_true = np.asarray([3, -0.5, 2, 7])\n",
    "y_pred = np.asarray([2.5, 0.0, 2, 8])\n",
    "numerator = np.var(y_pred - y_true)\n",
    "denominator = np.var(y_true)\n",
    "print numerator \n",
    "print denominator\n",
    "1 -  numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3125\n",
      "7.296875\n",
      "0.957173447537\n"
     ]
    }
   ],
   "source": [
    "y_diff_avg = np.average(y_true - y_pred)\n",
    "# 差自体の分散\n",
    "numerator = np.average((y_true - y_pred - y_diff_avg) ** 2)\n",
    "denominator = np.average((y_true - np.average(y_true)) ** 2)\n",
    "\n",
    "print numerator\n",
    "print denominator\n",
    "print 1 - numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948608137045\n",
      "0.948608137045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_true = np.asarray([3, -0.5, 2, 7])\n",
    "y_pred = np.asarray([2.5, 0.0, 2, 8])\n",
    "print r2_score(y_true, y_pred)\n",
    "\n",
    "# ターゲットと予測値対象物に対する分散を分子にするか\n",
    "numerator = np.average((y_true - y_pred) ** 2)\n",
    "denominator = np.average((y_true - np.average(y_true)) ** 2)\n",
    "print 1 - numerator/denominator\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93825665859564167"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "r2_score(y_true, y_pred, multioutput='variance_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93680052666227787"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "r2_score(y_true, y_pred, multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.96543779,  0.90816327])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_true, y_pred, multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92534562211981564"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_true, y_pred, multioutput=[0.3, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_criterion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-74ef14404e22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCRITERIA_REG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_criterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"friedman_mse\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_criterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFriedmanMSE\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_criterion' is not defined"
     ]
    }
   ],
   "source": [
    "CRITERIA_REG = {\"mse\": _criterion.MSE, \"friedman_mse\": _criterion.FriedmanMSE}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
